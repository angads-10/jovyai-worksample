# Architecture Overview

## System Architecture

The Decision Transformer implementation follows a modular architecture designed for offline reinforcement learning in healthcare settings.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Decision Transformer System              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Data Layer                                                 â”‚
â”‚  â”œâ”€â”€ MIMIC-III Processing  â”œâ”€â”€ D4RL Integration            â”‚
â”‚  â”œâ”€â”€ TrajectoryDataset     â”œâ”€â”€ Data Normalization          â”‚
â”‚  â””â”€â”€ DataLoaders           â””â”€â”€ Batch Processing            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Model Layer                                                â”‚
â”‚  â”œâ”€â”€ DecisionTransformer   â”œâ”€â”€ Positional Encoding         â”‚
â”‚  â”œâ”€â”€ Token Embeddings      â”œâ”€â”€ Causal Attention            â”‚
â”‚  â””â”€â”€ Action Prediction     â””â”€â”€ Loss Computation            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Training Layer                                             â”‚
â”‚  â”œâ”€â”€ Trainer               â”œâ”€â”€ Optimizer (AdamW)           â”‚
â”‚  â”œâ”€â”€ Scheduler             â”œâ”€â”€ Gradient Clipping           â”‚
â”‚  â””â”€â”€ Checkpointing         â””â”€â”€ Logging (TensorBoard/W&B)   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Evaluation Layer                                           â”‚
â”‚  â”œâ”€â”€ Behavior Cloning      â”œâ”€â”€ RTG Correlation             â”‚
â”‚  â”œâ”€â”€ Safety Violations     â”œâ”€â”€ Action Distribution         â”‚
â”‚  â””â”€â”€ Visualization         â””â”€â”€ Performance Metrics         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Deployment Layer                                           â”‚
â”‚  â”œâ”€â”€ API Fine-tuning       â”œâ”€â”€ JSONL Conversion            â”‚
â”‚  â”œâ”€â”€ OpenAI Integration    â”œâ”€â”€ SFT/RFT Pipeline            â”‚
â”‚  â””â”€â”€ Production Ready      â””â”€â”€ Configuration Management    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Model Architecture

### Decision Transformer Core

The Decision Transformer models sequential decision-making as a sequence modeling problem:

```
Input:  [RTGâ‚€, sâ‚€, aâ‚€, RTGâ‚, sâ‚, aâ‚, ..., RTGâ‚œ, sâ‚œ, aâ‚œ]
Output: [Ã¢â‚€, Ã¢â‚, ..., Ã¢â‚œ]
```

#### Component Breakdown

**1. Input Embeddings**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ RTG Embed   â”‚    â”‚State Embed  â”‚    â”‚Action Embed â”‚
â”‚   (1â†’d)     â”‚    â”‚ (s_dimâ†’d)   â”‚    â”‚(a_dimâ†’d)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**2. Token Type Embeddings**
- RTG tokens: Learnable embedding for return-to-go values
- State tokens: Learnable embedding for patient states
- Action tokens: Learnable embedding for medical actions

**3. Positional Encoding**
- Sinusoidal positional encoding for temporal information
- Enables model to understand sequence position

**4. Transformer Encoder**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Multi-Head Attention                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚   Query     â”‚  â”‚    Key      â”‚  â”‚   Value     â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚                        â†“                                â”‚
â”‚              Layer Normalization                        â”‚
â”‚                        â†“                                â”‚
â”‚              Feed-Forward Network                       â”‚
â”‚                        â†“                                â”‚
â”‚              Layer Normalization                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**5. Causal Attention Mask**
- Prevents future information leakage
- Ensures autoregressive property
- Critical for offline RL setting

**6. Action Prediction Head**
- Linear projection from model dimension to action space
- Predicts actions for each timestep

### Data Flow

```
Raw Data â†’ Preprocessing â†’ Tokenization â†’ Embeddings â†’ Transformer â†’ Predictions
    â†“           â†“            â†“            â†“            â†“           â†“
MIMIC/D4RL â†’ Normalize â†’ (R,s,a) â†’ Learnable â†’ Attention â†’ Actions
```

## Data Architecture

### Trajectory Representation

Each trajectory consists of:

```python
{
    'states': np.array,      # [T, state_dim] - Patient states over time
    'actions': np.array,     # [T, action_dim] - Medical interventions
    'rewards': np.array,     # [T] - Immediate rewards
    'rtgs': np.array,        # [T] - Return-to-go values
    'length': int,           # Actual trajectory length
    'trajectory_id': int     # Unique identifier
}
```

### State Representation (MIMIC-III)

```python
state_features = [
    'heart_rate',           # Vital signs
    'systolic_bp', 
    'diastolic_bp',
    'temperature',
    'respiratory_rate',
    'oxygen_saturation',
    'age',                  # Demographics
    'gender',
    'weight',
    'height',
    'glucose',              # Lab values
    'sodium',
    'potassium',
    'creatinine',
    'bun',
    'hemoglobin',
    'wbc_count'
]
```

### Action Representation (Medical Interventions)

```python
action_features = [
    'fluid_input',          # Fluid management
    'fluid_output',
    'vasopressor_dose',     # Medication
    'ventilation_mode',     # Respiratory support
    'fio2',
    'peep',
    'antibiotic_flag',      # Treatment flags
    'pain_med_flag',
    'sedation_flag'
]
```

## Training Architecture

### Training Loop

```
1. Load Batch â†’ 2. Forward Pass â†’ 3. Compute Loss â†’ 4. Backward Pass â†’ 5. Update
     â†“              â†“               â†“              â†“              â†“
  [s,a,r,rtg] â†’ Model(s,a,rtg) â†’ MSE(a,Ã¢) â†’ âˆ‡Loss â†’ Optimizer.step()
```

### Loss Function

The model uses Mean Squared Error (MSE) loss for action prediction:

```
â„’ = ğ”¼[âˆ‘â‚œ (aâ‚œ - Ã¢â‚œ)Â²]
```

Where:
- `aâ‚œ`: True action at timestep t
- `Ã¢â‚œ`: Predicted action at timestep t

### Optimization

- **Optimizer**: AdamW with weight decay
- **Scheduler**: Cosine annealing learning rate
- **Gradient Clipping**: Prevents exploding gradients
- **Regularization**: Dropout and weight decay

## Evaluation Architecture

### Metrics Pipeline

```
Model Predictions â†’ Evaluation Metrics â†’ Performance Analysis
        â†“                    â†“                    â†“
   Predicted Actions â†’ BC Accuracy â†’ Behavior Analysis
                    â†’ RTG Correlation â†’ Reward Understanding
                    â†’ Safety Violations â†’ Safety Analysis
                    â†’ Action Distribution â†’ Statistical Analysis
```

### Key Metrics

1. **Behavior Cloning Accuracy**
   - Percentage of predicted actions matching dataset
   - Threshold-based matching (e.g., within 0.1 units)

2. **RTG Correlation**
   - Correlation between predicted and actual returns
   - Measures reward understanding

3. **Safety Violation Rate**
   - Percentage of out-of-bounds actions
   - Critical for medical applications

4. **Action Distribution Analysis**
   - Statistical comparison of predicted vs. true actions
   - Identifies systematic biases

## Deployment Architecture

### API Integration Pipeline

```
Trajectories â†’ JSONL Conversion â†’ OpenAI API â†’ Fine-tuned Model
     â†“              â†“               â†“              â†“
Hospital Data â†’ SFT/RFT Format â†’ Upload/Job â†’ Deployed API
```

### Fine-tuning Formats

**SFT (Supervised Fine-Tuning)**
```json
{
  "messages": [
    {"role": "system", "content": "You are a medical AI..."},
    {"role": "user", "content": "Patient state: [vitals], RTG: 0.85"},
    {"role": "assistant", "content": "Action: [intervention]"}
  ]
}
```

**RFT (Reinforcement Fine-Tuning)**
```json
{
  "prompt": "Given patient trajectory, provide best intervention",
  "chosen": "Evidence-based treatment",
  "rejected": "Conservative observation"
}
```

## Configuration Architecture

### Hierarchical Configuration

```
Default Config â†’ Base Config â†’ Environment Config â†’ Runtime Config
     â†“              â†“              â†“                 â†“
   Hardcoded â†’ dt_base.yaml â†’ dt_hospital.yaml â†’ CLI Args
```

### Configuration Inheritance

- **Base**: General offline RL settings
- **Hospital**: Medical-specific parameters
- **Runtime**: Command-line overrides

## Error Handling Architecture

### Exception Hierarchy

```
Exception
â”œâ”€â”€ DataError
â”‚   â”œâ”€â”€ DatasetNotFoundError
â”‚   â”œâ”€â”€ InvalidFormatError
â”‚   â””â”€â”€ PreprocessingError
â”œâ”€â”€ ModelError
â”‚   â”œâ”€â”€ ArchitectureError
â”‚   â”œâ”€â”€ TrainingError
â”‚   â””â”€â”€ InferenceError
â””â”€â”€ DeploymentError
    â”œâ”€â”€ APIError
    â”œâ”€â”€ ConversionError
    â””â”€â”€ ValidationError
```

### Logging Strategy

- **Levels**: DEBUG, INFO, WARNING, ERROR, CRITICAL
- **Destinations**: Console, File, TensorBoard, Weights & Biases
- **Context**: Module, function, line number, timestamp

## Scalability Considerations

### Model Scalability

- **Parameter Count**: 100K to 1M+ parameters
- **Sequence Length**: Up to 1000+ timesteps
- **Batch Size**: Configurable based on GPU memory
- **Multi-GPU**: Ready for DataParallel/DistributedDataParallel

### Data Scalability

- **Dataset Size**: Handles datasets with millions of trajectories
- **Memory Management**: Streaming data loading for large datasets
- **Parallel Processing**: Multi-worker data loading
- **Caching**: Optional trajectory caching for repeated access

### Training Scalability

- **Distributed Training**: Ready for multi-node training
- **Mixed Precision**: FP16 support for memory efficiency
- **Gradient Accumulation**: Support for large effective batch sizes
- **Checkpointing**: Automatic checkpointing and resumption

## Security Architecture

### Data Privacy

- **Anonymization**: MIMIC-III data is already anonymized
- **Local Processing**: All processing happens locally
- **No Data Transmission**: No sensitive data sent to external APIs

### Model Security

- **Input Validation**: Comprehensive input sanitization
- **Output Validation**: Action bounds checking
- **Safety Monitors**: Built-in safety violation detection
- **Audit Trails**: Complete logging of model decisions

## Future Extensibility

### CQL Integration

The architecture is designed to easily integrate Conservative Q-Learning:

```python
# Future extension
class DecisionTransformerWithCQL(DecisionTransformer):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.cql_critic = CQLCritic(...)
    
    def compute_safety_loss(self, states, actions):
        return self.cql_critic.compute_conservative_loss(states, actions)
```

### Multi-task Learning

Ready for extension to multiple hospital departments:

```python
# Future extension
class MultiTaskDecisionTransformer(DecisionTransformer):
    def __init__(self, task_configs):
        self.task_heads = nn.ModuleDict({
            task: nn.Linear(d_model, action_dim) 
            for task in task_configs
        })
```

This modular architecture ensures the system is maintainable, extensible, and ready for production deployment in healthcare settings.
