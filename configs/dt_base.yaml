# Decision Transformer Base Configuration
# Standard configuration for general offline RL tasks

# Model Architecture
state_dim: 17                    # State dimension (default for MIMIC-like data)
action_dim: 9                    # Action dimension (default for hospital interventions)
d_model: 128                     # Model dimension (embedding size)
n_heads: 8                       # Number of attention heads
n_layers: 3                      # Number of transformer layers
max_length: 100                  # Maximum sequence length
dropout: 0.1                     # Dropout rate

# Training Configuration
learning_rate: 1e-4              # Learning rate
batch_size: 32                   # Batch size
num_epochs: 100                  # Number of training epochs
weight_decay: 1e-5               # Weight decay for regularization
grad_clip_norm: 1.0              # Gradient clipping norm

# Data Configuration
normalize: true                  # Whether to normalize states and actions
train_split: 0.8                 # Fraction of data for training
shuffle: true                    # Whether to shuffle training data

# Evaluation Configuration
eval_frequency: 10               # Evaluate every N epochs
save_frequency: 10               # Save checkpoint every N epochs
early_stopping_patience: 20      # Early stopping patience

# Logging Configuration
use_wandb: false                 # Whether to use Weights & Biases
log_level: "INFO"                # Logging level
save_predictions: true           # Whether to save predictions during eval

# Hardware Configuration
device: "cuda"                   # Device to use (cuda/cpu)
num_workers: 4                   # Number of data loading workers
pin_memory: true                 # Whether to pin memory for faster GPU transfer

# Model Checkpointing
checkpoint_dir: "checkpoints"    # Directory for model checkpoints
best_model_name: "best_model.pt" # Name for best model checkpoint
last_model_name: "last_model.pt" # Name for last model checkpoint
